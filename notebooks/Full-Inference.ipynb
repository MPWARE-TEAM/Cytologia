{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f80d673-af0f-4517-aaa4-ebbbd8c492cf",
   "metadata": {},
   "source": [
    "# CytologIA Data Challenge - Inference notebook\n",
    "\n",
    "My approach leverages a two-stage modeling pipeline to effectively detect and classify white blood cells. Stage 1 is a universal White Blood Cell Detection. This stage employs an object detection model to identify white blood cells, irrespective of their underlying class. The goal is to robustly locate and distinguish white blood cells from other elements in the image. Stage 2 is classification once white blood cells are detected in the first stage, a second-stage model classifies them into specific categories. Alternatively, this stage can directly learn class labels when working with full images in the absence of explicit white cell detections. This modular pipeline ensures accurate localization and precise classification, optimizing performance for varying data scenarios and improving the adaptability of the solution.\n",
    "\n",
    "<i>About inference timing (extract from online requirements):\n",
    "Each image must be processed with an inference time of less than 500 ms. While GPU usage is permitted to accelerate processing, the solution must be compatible with regular GPUs typically available in standard setups, without requiring specialized hardware like high-end server-grade GPUs. Solutions optimized for CPU-based inference are preferred to ensure broader applicability, but CPU optimization is not mandatory.</i>\n",
    "\n",
    "- Data: 20751 RGB images, 22689 boxes to submit (i.e maximum inference time allowed is around **2h52min**)\n",
    "- Hardware (standard): Single RTX3090/24GBVRAM - SSD disk - 48GB RAM - Intel CPUCoreI9.\n",
    "- Software: Python 3.10 / Pytorch 2.5 / Timm.\n",
    "- Models: YoloX, CNN, Transformers.\n",
    "- Licenses: All libraries and models weights are under open source Apache2 license.\n",
    "\n",
    "Source code and trained weights to use for this inference notebook can be downloaded from: https://cytologia.s3.amazonaws.com/submission_inference_6.zip\n",
    "- Test images must be under TRUSTII/images_cytologia folder\n",
    "- Test CSV file must be under TRUSTII/test.csv\n",
    "- CV: 0.93125\n",
    "- Public LB: 0.93785\n",
    "- Private LB: 0.93716\n",
    "\n",
    "### Stage1: Bounding Box detection (white blood cells) - Total time = 1h\n",
    "- Execute YoloX models\n",
    "- Apply weighted boxes fusion to merge predictions\n",
    "- Fit predictions to submission format\n",
    "- Dump predictions in a folder as PNG images\n",
    "\n",
    "### Stage2: White blood cells classification - Total time = 1h 41min\n",
    "- Execute multi-classes models (on detected bounding boxes)\n",
    "- Execute multi-labels models (on full images)\n",
    "- Ensemble multi-classes and multi-labels models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "556b7aff-f639-4737-8cc7-a88d0b9dd6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run this cell and restart kernel and run it again if you get an error about missing YoloX installation, it will work on the second run.\n",
    "# !pip install loguru\n",
    "# !pip install seaborn\n",
    "# !pip install scikit-learn\n",
    "# !pip install wandb\n",
    "# !pip install pytorch-lightning\n",
    "# !pip install timm\n",
    "# !pip install albumentations\n",
    "# !pip install scikit-image\n",
    "# !pip install pyarrow\n",
    "# !cd code/src_object_detector/YOLOX; pip install -v -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50f26c78-127c-4e0e-84db-ecf96d54ec7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mpware/miniconda3/envs/ml310/lib/python3.10/site-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 1.4.24 (you have 1.4.22). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"./code/src_object_detector/YOLOX\")  # sys.path.append(\"../code/src_object_detector/YOLOX\")\n",
    "sys.path.append(\"./code/src\")  # sys.path.append(\"../code/src\")\n",
    "import cdc\n",
    "from cdc.common.utils import *\n",
    "from cdc.common.constants import *\n",
    "from cdc.models.pl.classifier import *\n",
    "from cdc.models.pl.dataset import *\n",
    "from cdc.utils.imaging import *\n",
    "from cdc.script.inferv1 import *\n",
    "from cdc.yolo.tools import *\n",
    "from cdc.yolo.inference import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63940f7c-2a5d-469d-92e0-e72939f2d502",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os, time, random, gc, sys, math, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "sns.set(style='whitegrid', rc={\"grid.linewidth\": 0.1})\n",
    "sns.set_context(\"paper\", font_scale=0.8) \n",
    "import PIL\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "import wandb\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import pytorch_lightning as L\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "from pytorch_lightning import seed_everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0730436-1e99-494f-a46f-d027282c143b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0]\n",
      "Numpy 1.26.4\n",
      "Pandas 2.2.2\n",
      "Torch 2.5.1+cu124\n",
      "Transformers 4.44.2\n",
      "Lightning 2.4.0\n",
      "Albumentations 1.4.22\n",
      "CDC 1.0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Python\", sys.version)\n",
    "print(\"Numpy\", np.__version__)\n",
    "print(\"Pandas\", pd.__version__)\n",
    "print(\"Torch\", torch.__version__)\n",
    "print(\"Transformers\", transformers.__version__)\n",
    "print(\"Lightning\", L.__version__)\n",
    "print(\"Albumentations\", A.__version__)\n",
    "print(\"CDC\", cdc.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5599cb06-b662-437b-8e8d-32ff1788a954",
   "metadata": {},
   "source": [
    "## Stage1: WBC bounding boxes detection\n",
    "- Execute Yolo models\n",
    "- Apply weighted boxes fusion to merge predictions\n",
    "- Fit predictions to submission format\n",
    "- Dump predictions in a folder as PNG images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "191dae75-c947-4d90-8e41-10e89af8e1ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.01 ms, sys: 1.18 ms, total: 4.19 ms\n",
      "Wall time: 16.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "SEEDS = [42]\n",
    "seed_everything_now(SEEDS[0])\n",
    "seed_everything(SEEDS[0], workers=True)\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "DATA_ROOT = \"./data\" # \"../data\"\n",
    "TRUSTII_HOME = os.path.join(DATA_ROOT, \"TRUSTII\")  # DATA_ROOT\n",
    "TEST_FILE = os.path.join(TRUSTII_HOME, \"test.csv\")\n",
    "TEST_HOME = os.path.join(TRUSTII_HOME, \"images_cytologia\")\n",
    "MODELS_HOME = \"./models\" #  \"../models\"\n",
    "\n",
    "VERSION = \"v3.1\"\n",
    "INFERENCE_NAME = \"top1_inference_yolox%s_5models\"%VERSION\n",
    "MODEL_YOLOX_HOME = \"./code/src_object_detector/YOLOX/YOLOX_outputs\"  # \"../code/src_object_detector/YOLOX/YOLOX_outputs\"\n",
    "IMG_SIZE = 512 if VERSION == \"v3.1\" else 640\n",
    "DEVICE = \"gpu\" # \"cpu\"\n",
    "TEST_CONFIDENCE = 0.001\n",
    "PADDING_POLICY = \"full\"\n",
    "WBF_IOU = 0.25\n",
    "ROOT = \"yolox_s\"\n",
    "NMS_THRESHOLD = 0.30\n",
    "\n",
    "# Dump BBX\n",
    "CROP_MARGINS = 16\n",
    "CROP_FILE_TEST_CLEANED = os.path.join(TRUSTII_HOME, \"boxes_%s\"%INFERENCE_NAME, \"%s_%d_%s_%.4f_%.3f_%.3f_%s_m%d_seed42_test_cleaned.parquet\"%(ROOT, IMG_SIZE, VERSION, TEST_CONFIDENCE, NMS_THRESHOLD, WBF_IOU, PADDING_POLICY, CROP_MARGINS))\n",
    "CROP_HOME_TEST = os.path.join(TRUSTII_HOME, \"boxes_%s\"%INFERENCE_NAME, \"%s_%d_%s_%.4f_%.3f_%.3f_%s_m%d_seed42_test_cleaned\"%(ROOT, IMG_SIZE, VERSION, TEST_CONFIDENCE, NMS_THRESHOLD, WBF_IOU, PADDING_POLICY, CROP_MARGINS))\n",
    "os.makedirs(CROP_HOME_TEST, exist_ok=True)\n",
    "\n",
    "MODELS = {\n",
    "    \"yolox_s_%s_%d_seed_42\"%(VERSION, IMG_SIZE): {\n",
    "        \"fold0\":  (MODEL_YOLOX_HOME + \"/yolox_s_%s_%d_seed_42_fold0\"%(VERSION.replace(\".\", \"\"), IMG_SIZE), IMG_SIZE),\n",
    "        \"fold1\":  (MODEL_YOLOX_HOME + \"/yolox_s_%s_%d_seed_42_fold1\"%(VERSION.replace(\".\", \"\"), IMG_SIZE), IMG_SIZE),\n",
    "        \"fold2\":  (MODEL_YOLOX_HOME + \"/yolox_s_%s_%d_seed_42_fold2\"%(VERSION.replace(\".\", \"\") ,IMG_SIZE), IMG_SIZE),\n",
    "        \"fold3\":  (MODEL_YOLOX_HOME + \"/yolox_s_%s_%d_seed_42_fold3\"%(VERSION.replace(\".\", \"\"), IMG_SIZE), IMG_SIZE),\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45021859-4972-40c5-af46-f8e988cc665f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path: ./code/src_object_detector/YOLOX/YOLOX_outputs/yolox_s_v31_512_seed_42_fold0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/mpware/Documents/NOTEBOOKS/CDC/./code/src/cdc/yolo/inference.py:117: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(ckpt_file, map_location=\"cpu\")\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22689/22689 [12:11<00:00, 31.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path: ./code/src_object_detector/YOLOX/YOLOX_outputs/yolox_s_v31_512_seed_42_fold1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22689/22689 [11:18<00:00, 33.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path: ./code/src_object_detector/YOLOX/YOLOX_outputs/yolox_s_v31_512_seed_42_fold2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22689/22689 [12:03<00:00, 31.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path: ./code/src_object_detector/YOLOX/YOLOX_outputs/yolox_s_v31_512_seed_42_fold3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22689/22689 [12:08<00:00, 31.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(142213, 13)\n",
      "CPU times: user 32min 12s, sys: 7min 2s, total: 39min 15s\n",
      "Wall time: 47min 47s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>slide_width</th>\n",
       "      <th>slide_height</th>\n",
       "      <th>bbx_xtl</th>\n",
       "      <th>bbx_ytl</th>\n",
       "      <th>bbx_xbr</th>\n",
       "      <th>bbx_ybr</th>\n",
       "      <th>score</th>\n",
       "      <th>class</th>\n",
       "      <th>roi_width</th>\n",
       "      <th>roi_height</th>\n",
       "      <th>roi_surface</th>\n",
       "      <th>roi_surface_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000455d4-8.jpg</td>\n",
       "      <td>360</td>\n",
       "      <td>363</td>\n",
       "      <td>93.142822</td>\n",
       "      <td>97.396729</td>\n",
       "      <td>265.869141</td>\n",
       "      <td>265.160156</td>\n",
       "      <td>0.967818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>172.726318</td>\n",
       "      <td>167.763428</td>\n",
       "      <td>28977.160156</td>\n",
       "      <td>22.174135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000455d4-8.jpg</td>\n",
       "      <td>360</td>\n",
       "      <td>363</td>\n",
       "      <td>235.205566</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>325.601074</td>\n",
       "      <td>6.558105</td>\n",
       "      <td>0.002620</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.395508</td>\n",
       "      <td>6.558105</td>\n",
       "      <td>592.823303</td>\n",
       "      <td>0.453645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0007ccec-2.jpg</td>\n",
       "      <td>360</td>\n",
       "      <td>367</td>\n",
       "      <td>91.078003</td>\n",
       "      <td>82.700439</td>\n",
       "      <td>272.203613</td>\n",
       "      <td>283.134766</td>\n",
       "      <td>0.973170</td>\n",
       "      <td>0.0</td>\n",
       "      <td>181.125610</td>\n",
       "      <td>200.434326</td>\n",
       "      <td>36303.789062</td>\n",
       "      <td>27.477891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00080027-c.jpg</td>\n",
       "      <td>368</td>\n",
       "      <td>369</td>\n",
       "      <td>111.348633</td>\n",
       "      <td>106.664062</td>\n",
       "      <td>257.471191</td>\n",
       "      <td>261.254883</td>\n",
       "      <td>0.976595</td>\n",
       "      <td>0.0</td>\n",
       "      <td>146.122559</td>\n",
       "      <td>154.590820</td>\n",
       "      <td>22589.207031</td>\n",
       "      <td>16.635153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00084489-e.jpg</td>\n",
       "      <td>368</td>\n",
       "      <td>370</td>\n",
       "      <td>117.792969</td>\n",
       "      <td>114.902344</td>\n",
       "      <td>239.921875</td>\n",
       "      <td>284.726562</td>\n",
       "      <td>0.896660</td>\n",
       "      <td>0.0</td>\n",
       "      <td>122.128906</td>\n",
       "      <td>169.824219</td>\n",
       "      <td>20740.445312</td>\n",
       "      <td>15.232407</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         filename  slide_width  slide_height     bbx_xtl     bbx_ytl  \\\n",
       "0  000455d4-8.jpg          360           363   93.142822   97.396729   \n",
       "1  000455d4-8.jpg          360           363  235.205566    0.000000   \n",
       "2  0007ccec-2.jpg          360           367   91.078003   82.700439   \n",
       "3  00080027-c.jpg          368           369  111.348633  106.664062   \n",
       "4  00084489-e.jpg          368           370  117.792969  114.902344   \n",
       "\n",
       "      bbx_xbr     bbx_ybr     score  class   roi_width  roi_height  \\\n",
       "0  265.869141  265.160156  0.967818    0.0  172.726318  167.763428   \n",
       "1  325.601074    6.558105  0.002620    0.0   90.395508    6.558105   \n",
       "2  272.203613  283.134766  0.973170    0.0  181.125610  200.434326   \n",
       "3  257.471191  261.254883  0.976595    0.0  146.122559  154.590820   \n",
       "4  239.921875  284.726562  0.896660    0.0  122.128906  169.824219   \n",
       "\n",
       "    roi_surface  roi_surface_ratio  \n",
       "0  28977.160156          22.174135  \n",
       "1    592.823303           0.453645  \n",
       "2  36303.789062          27.477891  \n",
       "3  22589.207031          16.635153  \n",
       "4  20740.445312          15.232407  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Execute BBx models\n",
    "test_pd = pd.read_csv(TEST_FILE)\n",
    "files = [os.path.join(TEST_HOME, f) for f in test_pd[\"NAME\"].values]\n",
    "for name, _ in MODELS.items():\n",
    "    test_roi_predictions = []        \n",
    "    for fold, info in MODELS[name].items():\n",
    "        path, imgsize = info\n",
    "        print(\"Path:\", path)\n",
    "        if \"yolox\" in path:\n",
    "            roi_predictions_ = predict_yolox(None, path + \"/best_ckpt.pth\", test_conf=TEST_CONFIDENCE, nmsthre=NMS_THRESHOLD, image_size=imgsize, files=files, device=DEVICE)\n",
    "        else:\n",
    "            roi_predictions_ = predict_yolo(None, path + \"/weights/best.pt\", test_conf=TEST_CONFIDENCE, nmsthre=NMS_THRESHOLD, image_size=imgsize, files=files, device=DEVICE)                \n",
    "        test_roi_predictions.append(roi_predictions_)\n",
    "    test_roi_predictions = pd.concat(test_roi_predictions, ignore_index=True)\n",
    "test_roi_predictions_pd = test_roi_predictions.copy()\n",
    "test_roi_predictions_pd[\"roi_width\"] = test_roi_predictions_pd[\"bbx_xbr\"] - test_roi_predictions_pd[\"bbx_xtl\"]\n",
    "test_roi_predictions_pd[\"roi_height\"] = test_roi_predictions_pd[\"bbx_ybr\"] - test_roi_predictions_pd[\"bbx_ytl\"]\n",
    "test_roi_predictions_pd[\"roi_surface\"] = test_roi_predictions_pd[\"roi_width\"]*test_roi_predictions_pd[\"roi_height\"]\n",
    "test_roi_predictions_pd[\"roi_surface_ratio\"] = test_roi_predictions_pd[\"roi_surface\"]*100./(test_roi_predictions_pd[\"slide_width\"]*test_roi_predictions_pd[\"slide_height\"])\n",
    "print(test_roi_predictions_pd.shape)\n",
    "test_roi_predictions_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63faefb4-b7f4-493d-87d6-a09f1be23a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mpware/miniconda3/envs/ml310/lib/python3.10/site-packages/ensemble_boxes/ensemble_boxes_wbf.py:54: UserWarning: Y1 < 0 in box. Set it to 0.\n",
      "  warnings.warn('Y1 < 0 in box. Set it to 0.')\n",
      "/home/mpware/miniconda3/envs/ml310/lib/python3.10/site-packages/ensemble_boxes/ensemble_boxes_wbf.py:42: UserWarning: X1 < 0 in box. Set it to 0.\n",
      "  warnings.warn('X1 < 0 in box. Set it to 0.')\n",
      "/home/mpware/miniconda3/envs/ml310/lib/python3.10/site-packages/ensemble_boxes/ensemble_boxes_wbf.py:63: UserWarning: Y2 > 1 in box. Set it to 1. Check that you normalize boxes in [0, 1] range.\n",
      "  warnings.warn('Y2 > 1 in box. Set it to 1. Check that you normalize boxes in [0, 1] range.')\n",
      "/home/mpware/miniconda3/envs/ml310/lib/python3.10/site-packages/ensemble_boxes/ensemble_boxes_wbf.py:51: UserWarning: X2 > 1 in box. Set it to 1. Check that you normalize boxes in [0, 1] range.\n",
      "  warnings.warn('X2 > 1 in box. Set it to 1. Check that you normalize boxes in [0, 1] range.')\n",
      "<timed exec>:11: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29460, 14)\n",
      "CPU times: user 10.7 s, sys: 6.3 ms, total: 10.7 s\n",
      "Wall time: 10.7 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>slide_width</th>\n",
       "      <th>slide_height</th>\n",
       "      <th>bbx_xtl</th>\n",
       "      <th>bbx_ytl</th>\n",
       "      <th>bbx_xbr</th>\n",
       "      <th>bbx_ybr</th>\n",
       "      <th>score</th>\n",
       "      <th>class</th>\n",
       "      <th>roi_width</th>\n",
       "      <th>roi_height</th>\n",
       "      <th>roi_surface</th>\n",
       "      <th>roi_surface_ratio</th>\n",
       "      <th>predict_bb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000455d4-8.jpg</td>\n",
       "      <td>360</td>\n",
       "      <td>363</td>\n",
       "      <td>93</td>\n",
       "      <td>98</td>\n",
       "      <td>266</td>\n",
       "      <td>265</td>\n",
       "      <td>0.967940</td>\n",
       "      <td>0.0</td>\n",
       "      <td>173</td>\n",
       "      <td>167</td>\n",
       "      <td>28891</td>\n",
       "      <td>22.108203</td>\n",
       "      <td>(93.0, 266.0, 98.0, 265.0, 0.0, 22.10820324456688, 0.9679403901100159)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000455d4-8.jpg</td>\n",
       "      <td>360</td>\n",
       "      <td>363</td>\n",
       "      <td>234</td>\n",
       "      <td>0</td>\n",
       "      <td>325</td>\n",
       "      <td>7</td>\n",
       "      <td>0.002020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>91</td>\n",
       "      <td>7</td>\n",
       "      <td>637</td>\n",
       "      <td>0.487450</td>\n",
       "      <td>(234.0, 325.0, 0.0, 7.0, 0.0, 0.4874502601775329, 0.002019635634496808)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0007ccec-2.jpg</td>\n",
       "      <td>360</td>\n",
       "      <td>367</td>\n",
       "      <td>91</td>\n",
       "      <td>82</td>\n",
       "      <td>272</td>\n",
       "      <td>283</td>\n",
       "      <td>0.971822</td>\n",
       "      <td>0.0</td>\n",
       "      <td>181</td>\n",
       "      <td>201</td>\n",
       "      <td>36381</td>\n",
       "      <td>27.536331</td>\n",
       "      <td>(91.0, 272.0, 82.0, 283.0, 0.0, 27.536330608537693, 0.9718216061592102)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00080027-c.jpg</td>\n",
       "      <td>368</td>\n",
       "      <td>369</td>\n",
       "      <td>111</td>\n",
       "      <td>107</td>\n",
       "      <td>257</td>\n",
       "      <td>261</td>\n",
       "      <td>0.976950</td>\n",
       "      <td>0.0</td>\n",
       "      <td>146</td>\n",
       "      <td>154</td>\n",
       "      <td>22484</td>\n",
       "      <td>16.557676</td>\n",
       "      <td>(111.0, 257.0, 107.0, 261.0, 0.0, 16.55767644632968, 0.9769502282142639)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00084489-e.jpg</td>\n",
       "      <td>368</td>\n",
       "      <td>370</td>\n",
       "      <td>118</td>\n",
       "      <td>115</td>\n",
       "      <td>241</td>\n",
       "      <td>269</td>\n",
       "      <td>0.879252</td>\n",
       "      <td>0.0</td>\n",
       "      <td>123</td>\n",
       "      <td>154</td>\n",
       "      <td>18942</td>\n",
       "      <td>13.911575</td>\n",
       "      <td>(118.0, 241.0, 115.0, 269.0, 0.0, 13.911574618096358, 0.8792518973350525)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         filename  slide_width  slide_height  bbx_xtl  bbx_ytl  bbx_xbr  \\\n",
       "0  000455d4-8.jpg          360           363       93       98      266   \n",
       "1  000455d4-8.jpg          360           363      234        0      325   \n",
       "2  0007ccec-2.jpg          360           367       91       82      272   \n",
       "3  00080027-c.jpg          368           369      111      107      257   \n",
       "4  00084489-e.jpg          368           370      118      115      241   \n",
       "\n",
       "   bbx_ybr     score  class  roi_width  roi_height  roi_surface  \\\n",
       "0      265  0.967940    0.0        173         167        28891   \n",
       "1        7  0.002020    0.0         91           7          637   \n",
       "2      283  0.971822    0.0        181         201        36381   \n",
       "3      261  0.976950    0.0        146         154        22484   \n",
       "4      269  0.879252    0.0        123         154        18942   \n",
       "\n",
       "   roi_surface_ratio  \\\n",
       "0          22.108203   \n",
       "1           0.487450   \n",
       "2          27.536331   \n",
       "3          16.557676   \n",
       "4          13.911575   \n",
       "\n",
       "                                                                  predict_bb  \n",
       "0     (93.0, 266.0, 98.0, 265.0, 0.0, 22.10820324456688, 0.9679403901100159)  \n",
       "1    (234.0, 325.0, 0.0, 7.0, 0.0, 0.4874502601775329, 0.002019635634496808)  \n",
       "2    (91.0, 272.0, 82.0, 283.0, 0.0, 27.536330608537693, 0.9718216061592102)  \n",
       "3   (111.0, 257.0, 107.0, 261.0, 0.0, 16.55767644632968, 0.9769502282142639)  \n",
       "4  (118.0, 241.0, 115.0, 269.0, 0.0, 13.911574618096358, 0.8792518973350525)  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Run Weighted-Boxes-Fusion over the predictions\n",
    "roi_predictions_wbf = run_wbf(test_roi_predictions_pd, iou_thr=WBF_IOU, skip_box_thr=0.0001)\n",
    "roi_predictions_wbf[\"bbx_xtl\"] = roi_predictions_wbf[\"bbx_xtl\"].apply(lambda x: np.round(x)).astype(np.int32)\n",
    "roi_predictions_wbf[\"bbx_xbr\"] = roi_predictions_wbf[\"bbx_xbr\"].apply(lambda x: np.round(x)).astype(np.int32)\n",
    "roi_predictions_wbf[\"bbx_ybr\"] = roi_predictions_wbf[\"bbx_ybr\"].apply(lambda x: np.round(x)).astype(np.int32)\n",
    "roi_predictions_wbf[\"bbx_ytl\"] = roi_predictions_wbf[\"bbx_ytl\"].apply(lambda x: np.round(x)).astype(np.int32)\n",
    "roi_predictions_wbf[\"roi_width\"] = roi_predictions_wbf[\"bbx_xbr\"] - roi_predictions_wbf[\"bbx_xtl\"]\n",
    "roi_predictions_wbf[\"roi_height\"] = roi_predictions_wbf[\"bbx_ybr\"] - roi_predictions_wbf[\"bbx_ytl\"]\n",
    "roi_predictions_wbf[\"roi_surface\"] = roi_predictions_wbf[\"roi_width\"]*roi_predictions_wbf[\"roi_height\"]\n",
    "roi_predictions_wbf[\"roi_surface_ratio\"] = roi_predictions_wbf[\"roi_surface\"]*100./(roi_predictions_wbf[\"slide_width\"]*roi_predictions_wbf[\"slide_height\"])\n",
    "roi_predictions_wbf[\"predict_bb\"] = roi_predictions_wbf[[\"bbx_xtl\", \"bbx_xbr\", \"bbx_ytl\", \"bbx_ybr\", \"class\", \"roi_surface_ratio\", \"score\"]].apply(lambda x: (x[0], x[1], x[2], x[3], x[4], x[5], x[6]), axis=1)\n",
    "print(roi_predictions_wbf.shape)\n",
    "roi_predictions_wbf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2cf803a-566a-4b33-9594-bae3714765dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22689, 2)\n",
      "Too much predictions: (4497, 7)\n",
      "Not enough predictions: (10, 7)\n",
      "Equal predictions: (16244, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:13: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too much predictions: (0, 8)\n",
      "Not enough predictions: (0, 8)\n",
      "Equal predictions: (20751, 8)\n",
      "CPU times: user 1.27 s, sys: 9.65 ms, total: 1.28 s\n",
      "Wall time: 1.28 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NAME</th>\n",
       "      <th>slide_width</th>\n",
       "      <th>slide_height</th>\n",
       "      <th>predict_bb</th>\n",
       "      <th>predict_bbs</th>\n",
       "      <th>trustii_ids</th>\n",
       "      <th>trustii_bbs</th>\n",
       "      <th>predict_score_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000455d4-8.jpg</td>\n",
       "      <td>360</td>\n",
       "      <td>363</td>\n",
       "      <td>[(93.0, 266.0, 98.0, 265.0, 0.0, 22.10820324456688, 0.9679403901100159)]</td>\n",
       "      <td>1</td>\n",
       "      <td>[23798]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.967940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0007ccec-2.jpg</td>\n",
       "      <td>360</td>\n",
       "      <td>367</td>\n",
       "      <td>[(91.0, 272.0, 82.0, 283.0, 0.0, 27.536330608537693, 0.9718216061592102)]</td>\n",
       "      <td>1</td>\n",
       "      <td>[22386]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.971822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00080027-c.jpg</td>\n",
       "      <td>368</td>\n",
       "      <td>369</td>\n",
       "      <td>[(111.0, 257.0, 107.0, 261.0, 0.0, 16.55767644632968, 0.9769502282142639)]</td>\n",
       "      <td>1</td>\n",
       "      <td>[59769]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.976950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00084489-e.jpg</td>\n",
       "      <td>368</td>\n",
       "      <td>370</td>\n",
       "      <td>[(118.0, 241.0, 115.0, 269.0, 0.0, 13.911574618096358, 0.8792518973350525)]</td>\n",
       "      <td>1</td>\n",
       "      <td>[61484]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.879252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000cfe84-e.jpg</td>\n",
       "      <td>352</td>\n",
       "      <td>357</td>\n",
       "      <td>[(100.0, 251.0, 91.0, 264.0, 0.0, 20.787974280621338, 0.9656392931938171)]</td>\n",
       "      <td>1</td>\n",
       "      <td>[36896]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.965639</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             NAME  slide_width  slide_height  \\\n",
       "0  000455d4-8.jpg          360           363   \n",
       "1  0007ccec-2.jpg          360           367   \n",
       "2  00080027-c.jpg          368           369   \n",
       "3  00084489-e.jpg          368           370   \n",
       "4  000cfe84-e.jpg          352           357   \n",
       "\n",
       "                                                                    predict_bb  \\\n",
       "0     [(93.0, 266.0, 98.0, 265.0, 0.0, 22.10820324456688, 0.9679403901100159)]   \n",
       "1    [(91.0, 272.0, 82.0, 283.0, 0.0, 27.536330608537693, 0.9718216061592102)]   \n",
       "2   [(111.0, 257.0, 107.0, 261.0, 0.0, 16.55767644632968, 0.9769502282142639)]   \n",
       "3  [(118.0, 241.0, 115.0, 269.0, 0.0, 13.911574618096358, 0.8792518973350525)]   \n",
       "4   [(100.0, 251.0, 91.0, 264.0, 0.0, 20.787974280621338, 0.9656392931938171)]   \n",
       "\n",
       "   predict_bbs trustii_ids  trustii_bbs  predict_score_avg  \n",
       "0            1     [23798]            1           0.967940  \n",
       "1            1     [22386]            1           0.971822  \n",
       "2            1     [59769]            1           0.976950  \n",
       "3            1     [61484]            1           0.879252  \n",
       "4            1     [36896]            1           0.965639  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "print(test_pd.shape)\n",
    "test_image_pd = test_pd.groupby(\"NAME\")[[\"trustii_id\"]].agg(trustii_ids=(\"trustii_id\", list), trustii_bbs=(\"trustii_id\", 'count')).reset_index()\n",
    "# Fit to trustii IDs\n",
    "roi_image_predictions_wbf = roi_predictions_wbf.sort_values([\"filename\", \"score\", \"roi_surface_ratio\"], ascending=[True, False, False]).reset_index(drop=True)\n",
    "roi_image_predictions_wbf = roi_image_predictions_wbf.groupby([\"filename\", \"slide_width\", \"slide_height\"])[[\"predict_bb\"]].agg(list).reset_index().rename(columns={'filename':'NAME'})\n",
    "roi_image_predictions_wbf[\"predict_bbs\"] = roi_image_predictions_wbf[\"predict_bb\"].apply(lambda x: len(x))\n",
    "roi_image_predictions_wbf = pd.merge(roi_image_predictions_wbf, test_image_pd, on=\"NAME\", how=\"inner\")\n",
    "print(\"Too much predictions:\", roi_image_predictions_wbf[roi_image_predictions_wbf[\"predict_bbs\"] > roi_image_predictions_wbf[\"trustii_bbs\"]].shape)\n",
    "print(\"Not enough predictions:\", roi_image_predictions_wbf[roi_image_predictions_wbf[\"predict_bbs\"] < roi_image_predictions_wbf[\"trustii_bbs\"]].shape)\n",
    "print(\"Equal predictions:\", roi_image_predictions_wbf[roi_image_predictions_wbf[\"predict_bbs\"] == roi_image_predictions_wbf[\"trustii_bbs\"]].shape)\n",
    "not_enough_predict = roi_image_predictions_wbf[roi_image_predictions_wbf[\"predict_bbs\"] < roi_image_predictions_wbf[\"trustii_bbs\"]][\"NAME\"].unique()\n",
    "# Keep top N predictions based on confidence.\n",
    "roi_image_predictions_wbf[\"predict_bb\"] = roi_image_predictions_wbf[[\"predict_bb\", \"trustii_bbs\", \"slide_width\", \"slide_height\"]].apply(lambda x: fit_format(x[0], x[1], x[2], x[3], padding=True, padding_policy=PADDING_POLICY), axis=1)\n",
    "roi_image_predictions_wbf[\"predict_bbs\"] = roi_image_predictions_wbf[\"predict_bb\"].apply(lambda x: len(x))\n",
    "roi_image_predictions_wbf[\"predict_score_avg\"] = roi_image_predictions_wbf[\"predict_bb\"].apply(lambda x: compute_bbx_avg_score(x))\n",
    "print(\"Too much predictions:\", roi_image_predictions_wbf[roi_image_predictions_wbf[\"predict_bbs\"] > roi_image_predictions_wbf[\"trustii_bbs\"]].shape)\n",
    "print(\"Not enough predictions:\", roi_image_predictions_wbf[roi_image_predictions_wbf[\"predict_bbs\"] < roi_image_predictions_wbf[\"trustii_bbs\"]].shape)\n",
    "print(\"Equal predictions:\", roi_image_predictions_wbf[roi_image_predictions_wbf[\"predict_bbs\"] == roi_image_predictions_wbf[\"trustii_bbs\"]].shape)\n",
    "roi_image_predictions_wbf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a0454f9-4d83-4a88-9b8b-34b04d892e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22689/22689 [09:58<00:00, 37.89it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NAME</th>\n",
       "      <th>img_width</th>\n",
       "      <th>img_height</th>\n",
       "      <th>predict_score_avg</th>\n",
       "      <th>predict_bb</th>\n",
       "      <th>trustii_id</th>\n",
       "      <th>pred_x1</th>\n",
       "      <th>pred_y1</th>\n",
       "      <th>pred_x2</th>\n",
       "      <th>pred_y2</th>\n",
       "      <th>pred_score</th>\n",
       "      <th>pred_width</th>\n",
       "      <th>pred_height</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000455d4-8.jpg</td>\n",
       "      <td>360</td>\n",
       "      <td>363</td>\n",
       "      <td>0.967940</td>\n",
       "      <td>(93.0, 266.0, 98.0, 265.0, 0.0, 22.10820324456688, 0.9679403901100159)</td>\n",
       "      <td>23798</td>\n",
       "      <td>93</td>\n",
       "      <td>98</td>\n",
       "      <td>266</td>\n",
       "      <td>265</td>\n",
       "      <td>0.967940</td>\n",
       "      <td>173</td>\n",
       "      <td>167</td>\n",
       "      <td>000455d4-8-23798.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0007ccec-2.jpg</td>\n",
       "      <td>360</td>\n",
       "      <td>367</td>\n",
       "      <td>0.971822</td>\n",
       "      <td>(91.0, 272.0, 82.0, 283.0, 0.0, 27.536330608537693, 0.9718216061592102)</td>\n",
       "      <td>22386</td>\n",
       "      <td>91</td>\n",
       "      <td>82</td>\n",
       "      <td>272</td>\n",
       "      <td>283</td>\n",
       "      <td>0.971822</td>\n",
       "      <td>181</td>\n",
       "      <td>201</td>\n",
       "      <td>0007ccec-2-22386.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00080027-c.jpg</td>\n",
       "      <td>368</td>\n",
       "      <td>369</td>\n",
       "      <td>0.976950</td>\n",
       "      <td>(111.0, 257.0, 107.0, 261.0, 0.0, 16.55767644632968, 0.9769502282142639)</td>\n",
       "      <td>59769</td>\n",
       "      <td>111</td>\n",
       "      <td>107</td>\n",
       "      <td>257</td>\n",
       "      <td>261</td>\n",
       "      <td>0.976950</td>\n",
       "      <td>146</td>\n",
       "      <td>154</td>\n",
       "      <td>00080027-c-59769.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00084489-e.jpg</td>\n",
       "      <td>368</td>\n",
       "      <td>370</td>\n",
       "      <td>0.879252</td>\n",
       "      <td>(118.0, 241.0, 115.0, 269.0, 0.0, 13.911574618096358, 0.8792518973350525)</td>\n",
       "      <td>61484</td>\n",
       "      <td>118</td>\n",
       "      <td>115</td>\n",
       "      <td>241</td>\n",
       "      <td>269</td>\n",
       "      <td>0.879252</td>\n",
       "      <td>123</td>\n",
       "      <td>154</td>\n",
       "      <td>00084489-e-61484.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000cfe84-e.jpg</td>\n",
       "      <td>352</td>\n",
       "      <td>357</td>\n",
       "      <td>0.965639</td>\n",
       "      <td>(100.0, 251.0, 91.0, 264.0, 0.0, 20.787974280621338, 0.9656392931938171)</td>\n",
       "      <td>36896</td>\n",
       "      <td>100</td>\n",
       "      <td>91</td>\n",
       "      <td>251</td>\n",
       "      <td>264</td>\n",
       "      <td>0.965639</td>\n",
       "      <td>151</td>\n",
       "      <td>173</td>\n",
       "      <td>000cfe84-e-36896.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22684</th>\n",
       "      <td>fffff491-3.jpg</td>\n",
       "      <td>360</td>\n",
       "      <td>360</td>\n",
       "      <td>0.247647</td>\n",
       "      <td>(132.0, 242.0, 126.0, 244.0, 0.0, 10.015432098765432, 0.9708890914916992)</td>\n",
       "      <td>45140</td>\n",
       "      <td>132</td>\n",
       "      <td>126</td>\n",
       "      <td>242</td>\n",
       "      <td>244</td>\n",
       "      <td>0.970889</td>\n",
       "      <td>110</td>\n",
       "      <td>118</td>\n",
       "      <td>fffff491-3-45140.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22685</th>\n",
       "      <td>fffff491-3.jpg</td>\n",
       "      <td>360</td>\n",
       "      <td>360</td>\n",
       "      <td>0.247647</td>\n",
       "      <td>(0.0, 46.0, 0.0, 42.0, 0.0, 1.4907407407407407, 0.1545945703983307)</td>\n",
       "      <td>16280</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>42</td>\n",
       "      <td>0.154595</td>\n",
       "      <td>46</td>\n",
       "      <td>42</td>\n",
       "      <td>fffff491-3-16280.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22686</th>\n",
       "      <td>fffff491-3.jpg</td>\n",
       "      <td>360</td>\n",
       "      <td>360</td>\n",
       "      <td>0.247647</td>\n",
       "      <td>(334.0, 359.0, 3.0, 94.0, 0.0, 1.7554012345679013, 0.05295475572347641)</td>\n",
       "      <td>44984</td>\n",
       "      <td>334</td>\n",
       "      <td>3</td>\n",
       "      <td>359</td>\n",
       "      <td>94</td>\n",
       "      <td>0.052955</td>\n",
       "      <td>25</td>\n",
       "      <td>91</td>\n",
       "      <td>fffff491-3-44984.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22687</th>\n",
       "      <td>fffff491-3.jpg</td>\n",
       "      <td>360</td>\n",
       "      <td>360</td>\n",
       "      <td>0.247647</td>\n",
       "      <td>(277.0, 356.0, 63.0, 148.0, 0.0, 5.181327160493828, 0.037306852638721466)</td>\n",
       "      <td>41305</td>\n",
       "      <td>277</td>\n",
       "      <td>63</td>\n",
       "      <td>356</td>\n",
       "      <td>148</td>\n",
       "      <td>0.037307</td>\n",
       "      <td>79</td>\n",
       "      <td>85</td>\n",
       "      <td>fffff491-3-41305.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22688</th>\n",
       "      <td>fffff491-3.jpg</td>\n",
       "      <td>360</td>\n",
       "      <td>360</td>\n",
       "      <td>0.247647</td>\n",
       "      <td>(66.0, 129.0, 135.0, 244.0, 0.0, 5.298611111111111, 0.02249094843864441)</td>\n",
       "      <td>42293</td>\n",
       "      <td>66</td>\n",
       "      <td>135</td>\n",
       "      <td>129</td>\n",
       "      <td>244</td>\n",
       "      <td>0.022491</td>\n",
       "      <td>63</td>\n",
       "      <td>109</td>\n",
       "      <td>fffff491-3-42293.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22689 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 NAME  img_width  img_height  predict_score_avg  \\\n",
       "0      000455d4-8.jpg        360         363           0.967940   \n",
       "1      0007ccec-2.jpg        360         367           0.971822   \n",
       "2      00080027-c.jpg        368         369           0.976950   \n",
       "3      00084489-e.jpg        368         370           0.879252   \n",
       "4      000cfe84-e.jpg        352         357           0.965639   \n",
       "...               ...        ...         ...                ...   \n",
       "22684  fffff491-3.jpg        360         360           0.247647   \n",
       "22685  fffff491-3.jpg        360         360           0.247647   \n",
       "22686  fffff491-3.jpg        360         360           0.247647   \n",
       "22687  fffff491-3.jpg        360         360           0.247647   \n",
       "22688  fffff491-3.jpg        360         360           0.247647   \n",
       "\n",
       "                                                                      predict_bb  \\\n",
       "0         (93.0, 266.0, 98.0, 265.0, 0.0, 22.10820324456688, 0.9679403901100159)   \n",
       "1        (91.0, 272.0, 82.0, 283.0, 0.0, 27.536330608537693, 0.9718216061592102)   \n",
       "2       (111.0, 257.0, 107.0, 261.0, 0.0, 16.55767644632968, 0.9769502282142639)   \n",
       "3      (118.0, 241.0, 115.0, 269.0, 0.0, 13.911574618096358, 0.8792518973350525)   \n",
       "4       (100.0, 251.0, 91.0, 264.0, 0.0, 20.787974280621338, 0.9656392931938171)   \n",
       "...                                                                          ...   \n",
       "22684  (132.0, 242.0, 126.0, 244.0, 0.0, 10.015432098765432, 0.9708890914916992)   \n",
       "22685        (0.0, 46.0, 0.0, 42.0, 0.0, 1.4907407407407407, 0.1545945703983307)   \n",
       "22686    (334.0, 359.0, 3.0, 94.0, 0.0, 1.7554012345679013, 0.05295475572347641)   \n",
       "22687  (277.0, 356.0, 63.0, 148.0, 0.0, 5.181327160493828, 0.037306852638721466)   \n",
       "22688   (66.0, 129.0, 135.0, 244.0, 0.0, 5.298611111111111, 0.02249094843864441)   \n",
       "\n",
       "      trustii_id  pred_x1  pred_y1  pred_x2  pred_y2  pred_score  pred_width  \\\n",
       "0          23798       93       98      266      265    0.967940         173   \n",
       "1          22386       91       82      272      283    0.971822         181   \n",
       "2          59769      111      107      257      261    0.976950         146   \n",
       "3          61484      118      115      241      269    0.879252         123   \n",
       "4          36896      100       91      251      264    0.965639         151   \n",
       "...          ...      ...      ...      ...      ...         ...         ...   \n",
       "22684      45140      132      126      242      244    0.970889         110   \n",
       "22685      16280        0        0       46       42    0.154595          46   \n",
       "22686      44984      334        3      359       94    0.052955          25   \n",
       "22687      41305      277       63      356      148    0.037307          79   \n",
       "22688      42293       66      135      129      244    0.022491          63   \n",
       "\n",
       "       pred_height              filename  \n",
       "0              167  000455d4-8-23798.png  \n",
       "1              201  0007ccec-2-22386.png  \n",
       "2              154  00080027-c-59769.png  \n",
       "3              154  00084489-e-61484.png  \n",
       "4              173  000cfe84-e-36896.png  \n",
       "...            ...                   ...  \n",
       "22684          118  fffff491-3-45140.png  \n",
       "22685           42  fffff491-3-16280.png  \n",
       "22686           91  fffff491-3-44984.png  \n",
       "22687           85  fffff491-3-41305.png  \n",
       "22688          109  fffff491-3-42293.png  \n",
       "\n",
       "[22689 rows x 14 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 3s, sys: 17.1 s, total: 3min 20s\n",
      "Wall time: 9min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# To submission format\n",
    "submission_pd = roi_image_predictions_wbf[[\"NAME\", \"slide_width\", \"slide_height\", \"predict_score_avg\", \"predict_bb\", \"trustii_ids\"]].explode(['predict_bb', 'trustii_ids']).rename(columns={'trustii_ids':'trustii_id', 'slide_height':'img_height', 'slide_width': 'img_width'})\n",
    "submission_pd[\"pred_x1\"] = submission_pd[\"predict_bb\"].apply(lambda x: x[0]).astype(np.int32)\n",
    "submission_pd[\"pred_y1\"] = submission_pd[\"predict_bb\"].apply(lambda x: x[2]).astype(np.int32)\n",
    "submission_pd[\"pred_x2\"] = submission_pd[\"predict_bb\"].apply(lambda x: x[1]).astype(np.int32)\n",
    "submission_pd[\"pred_y2\"] = submission_pd[\"predict_bb\"].apply(lambda x: x[3]).astype(np.int32)\n",
    "submission_pd[\"pred_score\"] = submission_pd[\"predict_bb\"].apply(lambda x: x[-1])\n",
    "submission_pd[\"pred_width\"] = submission_pd[\"pred_x2\"] - submission_pd[\"pred_x1\"]\n",
    "submission_pd[\"pred_height\"] = submission_pd[\"pred_y2\"] - submission_pd[\"pred_y1\"]\n",
    "submission_pd = submission_pd.reset_index(drop=True)\n",
    "\n",
    "# Dump BBx in a folder\n",
    "dump_boxes(submission_pd, TEST_HOME, CROP_HOME_TEST, margins=CROP_MARGINS)\n",
    "submission_pd.to_parquet(CROP_FILE_TEST_CLEANED)\n",
    "display(submission_pd)\n",
    "\n",
    "# Debug only\n",
    "# submission_csv_pd = submission_pd[[\"trustii_id\", \"NAME\", \"pred_x1\", \"pred_y1\", \"pred_x2\", \"pred_y2\"]].copy().rename(columns={'pred_x1':'x1', 'pred_y1':'y1', 'pred_x2':'x2', 'pred_y2':'y2'})\n",
    "# submission_csv_pd[\"class\"] = \"PNN\"\n",
    "# # Keep same order\n",
    "# submission_csv_pd = pd.merge(test_pd, submission_csv_pd, on=[\"trustii_id\", \"NAME\"], how=\"left\")\n",
    "# submission_csv_pd.to_csv(CROP_FILE_TEST_CLEANED.replace(\".parquet\", \".csv\"), index=False)\n",
    "# submission_csv_pd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf943c0-77a6-4e6d-a9a9-74fdf152b795",
   "metadata": {},
   "source": [
    "## Stage2: White blood cells classification\n",
    "- Execute multi-classes models (on detected bounding boxes)\n",
    "- Execute multi-labels models (on full images)\n",
    "- Ensemble multi-classes and multi-labels models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e9a54dc-38a2-4a2e-92d6-f27c8c6147b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YoloX v3.1 - CV=0.9313, LB=0.93785\n",
    "models_dict = {\n",
    "\n",
    "    # Multiclasses models\n",
    "    'cnn_and_transformers-bb-multiclass': {\n",
    "        # CV=0.9182\n",
    "        'root_dir_mc_512/0.25': [\n",
    "            (f'{MODELS_HOME}/TRUSTII-RGB/timm_tf_efficientnetv2_m.in21k_512_None_v1.4.0-pl-crop-m16/stage2/seed42', None), # CV=0.9182, rd2, LB=0.9323 - 10min,\n",
    "        ],\n",
    "        # CV=0.9191 with Dino\n",
    "        'root_dir_mc_224/0.25': [\n",
    "            (f'{MODELS_HOME}/TRUSTII-RGB/timm_vit_large_patch16_224.augreg_in21k_ft_in1k_224_None_v1.2.5-pl-crop-m16/stage1/seed42', [[hflip]]), # with background, CV=0.9037 LB(HFlip)=0.9254 LB(NoTTA)=0.9260 - 16min\n",
    "            (f'{MODELS_HOME}/TRUSTII-RGB/timm_vit_large_patch16_224.augreg_in21k_ft_in1k_224_None_v1.2.4-pl-crop-m16/stage1/seed42', [[hflip]]), # CV=0.9061 LB(HFlip)=0.9265 LB(NoTTA)=0.9269 - 16min\n",
    "            (f'{MODELS_HOME}/TRUSTII-RGB/foundation_dinov2_vitb14_DinoBloom-B.pth_224_None_v1.4.0.6-pl-crop-m16/stage1/seed42', [[hflip]]),  # CV=0.9133, rd2, LB(HFlip)=0.9277 - 7min\n",
    "        ],\n",
    "        # CV=0.9165\n",
    "        'root_dir_mc_384/0.25': [\n",
    "            (f'{MODELS_HOME}/TRUSTII-RGB/timm_nextvit_large.bd_ssld_6m_in1k_384_384_None_v1.4.0-pl-crop-m16/stage2/seed42', [[hflip]]), # CV=0.9165 rd2, LB(HFlip)=0.9279 - 24min\n",
    "        ],\n",
    "        # CV=0.9181\n",
    "        'root_dir_mc_tr_512/0.25': [\n",
    "            (f'{MODELS_HOME}/TRUSTII-RGB/timm_tiny_vit_21m_512.dist_in22k_ft_in1k_512_None_v1.5.0-pl-crop-m16/stage1/seed42', None), # CV=0.9181 rd2, LB=0.9291 - 8min\n",
    "        ],          \n",
    "    },    \n",
    "\n",
    "    # Multilabels models\n",
    "    'cnn_and_transformers-multilabel': {\n",
    "        # CV=0.9279\n",
    "        'root_dir_ml_512/1.0': [\n",
    "            (f'{MODELS_HOME}/TRUSTII-RGB/timm_tf_efficientnetv2_m.in21k_512_None_v1.3.0-pl/stage3/seed42', [[hflip]]), # with background rd2, CV=0.9301/0.9279, HFlip - 20min            \n",
    "        ]\n",
    "    },\n",
    "}\n",
    "\n",
    "ALPHA = 0.457"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beef6f90-99e8-42c4-aaa7-b2c6052efccf",
   "metadata": {},
   "source": [
    "### Execute models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ec3d5d1-a28d-40f0-a395-dd46d7c8f518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing multi-classes model: {'cnn_and_transformers-bb-multiclass': {'root_dir': './models/TRUSTII-RGB/timm_tf_efficientnetv2_m.in21k_512_None_v1.4.0-pl-crop-m16/stage2/seed42'}} (22689, 14) TTA: None\n",
      "Loading: ./models/TRUSTII-RGB/timm_tf_efficientnetv2_m.in21k_512_None_v1.4.0-pl-crop-m16/stage2/seed42/fold0/best_epoch=21-val_f1=0.9192.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/mpware/Documents/NOTEBOOKS/CDC/./code/src/cdc/script/inferv1.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_dump = torch.load(best_weights, map_location='cpu')\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccd5ec6f8c69472e84b976cd1cd61a0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: ./models/TRUSTII-RGB/timm_tf_efficientnetv2_m.in21k_512_None_v1.4.0-pl-crop-m16/stage2/seed42/fold1/best_epoch=19-val_f1=0.9202.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/mpware/Documents/NOTEBOOKS/CDC/./code/src/cdc/script/inferv1.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_dump = torch.load(best_weights, map_location='cpu')\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0db409557f1f46d78136ad2b67434196",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: ./models/TRUSTII-RGB/timm_tf_efficientnetv2_m.in21k_512_None_v1.4.0-pl-crop-m16/stage2/seed42/fold2/best_epoch=23-val_f1=0.9187.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61fb46b5d7d64166b8832b3d5ca7ef00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: ./models/TRUSTII-RGB/timm_tf_efficientnetv2_m.in21k_512_None_v1.4.0-pl-crop-m16/stage2/seed42/fold3/best_epoch=22-val_f1=0.9150.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "775876d7ab9c47d38746047c45f96151",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 596.0879874229431\n",
      "Executing multi-classes model: {'cnn_and_transformers-bb-multiclass': {'root_dir': './models/TRUSTII-RGB/timm_vit_large_patch16_224.augreg_in21k_ft_in1k_224_None_v1.2.5-pl-crop-m16/stage1/seed42'}} (22689, 14) TTA: [[<function hflip at 0x7fae90bcb400>]]\n",
      "Loading: ./models/TRUSTII-RGB/timm_vit_large_patch16_224.augreg_in21k_ft_in1k_224_None_v1.2.5-pl-crop-m16/stage1/seed42/fold0/best_epoch=28-val_f1=0.9114.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/mpware/Documents/NOTEBOOKS/CDC/./code/src/cdc/script/inferv1.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_dump = torch.load(best_weights, map_location='cpu')\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble of 1 model(s), tta=[[<function hflip at 0x7fae90bcb400>]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "197e592316614747bfafd2c6474f8bd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: ./models/TRUSTII-RGB/timm_vit_large_patch16_224.augreg_in21k_ft_in1k_224_None_v1.2.5-pl-crop-m16/stage1/seed42/fold1/best_epoch=28-val_f1=0.9116.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble of 1 model(s), tta=[[<function hflip at 0x7fae90bcb400>]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6140f7733efb4bb3b7fd27dfb98108da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: ./models/TRUSTII-RGB/timm_vit_large_patch16_224.augreg_in21k_ft_in1k_224_None_v1.2.5-pl-crop-m16/stage1/seed42/fold2/best_epoch=30-val_f1=0.9037.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble of 1 model(s), tta=[[<function hflip at 0x7fae90bcb400>]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "008862babb7a4ebcb69cbb209ecbcf33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: ./models/TRUSTII-RGB/timm_vit_large_patch16_224.augreg_in21k_ft_in1k_224_None_v1.2.5-pl-crop-m16/stage1/seed42/fold3/best_epoch=31-val_f1=0.9040.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble of 1 model(s), tta=[[<function hflip at 0x7fae90bcb400>]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1f9741941af4f34bbf0d3906a648955",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 1004.2678382396698\n",
      "Executing multi-classes model: {'cnn_and_transformers-bb-multiclass': {'root_dir': './models/TRUSTII-RGB/timm_vit_large_patch16_224.augreg_in21k_ft_in1k_224_None_v1.2.4-pl-crop-m16/stage1/seed42'}} (22689, 14) TTA: [[<function hflip at 0x7fae90bcb400>]]\n",
      "Loading: ./models/TRUSTII-RGB/timm_vit_large_patch16_224.augreg_in21k_ft_in1k_224_None_v1.2.4-pl-crop-m16/stage1/seed42/fold0/best_epoch=28-val_f1=0.9027.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/mpware/Documents/NOTEBOOKS/CDC/./code/src/cdc/script/inferv1.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_dump = torch.load(best_weights, map_location='cpu')\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble of 1 model(s), tta=[[<function hflip at 0x7fae90bcb400>]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2c1936de9f9470cbf6d877f2781cbdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: ./models/TRUSTII-RGB/timm_vit_large_patch16_224.augreg_in21k_ft_in1k_224_None_v1.2.4-pl-crop-m16/stage1/seed42/fold1/best_epoch=31-val_f1=0.9135.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble of 1 model(s), tta=[[<function hflip at 0x7fae90bcb400>]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "678b6d0d584247d4bfe60e315ac045b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: ./models/TRUSTII-RGB/timm_vit_large_patch16_224.augreg_in21k_ft_in1k_224_None_v1.2.4-pl-crop-m16/stage1/seed42/fold2/best_epoch=28-val_f1=0.9042.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble of 1 model(s), tta=[[<function hflip at 0x7fae90bcb400>]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e43e57ede33479baa7fe8b3ae207b62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: ./models/TRUSTII-RGB/timm_vit_large_patch16_224.augreg_in21k_ft_in1k_224_None_v1.2.4-pl-crop-m16/stage1/seed42/fold3/best_epoch=31-val_f1=0.9037.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble of 1 model(s), tta=[[<function hflip at 0x7fae90bcb400>]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40fb3aa900214eb18a19051ee9bfcf11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 1005.8970401287079\n",
      "Executing multi-classes model: {'cnn_and_transformers-bb-multiclass': {'root_dir': './models/TRUSTII-RGB/foundation_dinov2_vitb14_DinoBloom-B.pth_224_None_v1.4.0.6-pl-crop-m16/stage1/seed42'}} (22689, 14) TTA: [[<function hflip at 0x7fae90bcb400>]]\n",
      "Loading: ./models/TRUSTII-RGB/foundation_dinov2_vitb14_DinoBloom-B.pth_224_None_v1.4.0.6-pl-crop-m16/stage1/seed42/fold0/best_epoch=28-val_f1=0.9137.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/mpware/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "/home/mpware/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is available (SwiGLU)\")\n",
      "/home/mpware/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)\n",
      "  warnings.warn(\"xFormers is available (Attention)\")\n",
      "/home/mpware/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)\n",
      "  warnings.warn(\"xFormers is available (Block)\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading (dinov2_vitb14) weights: DinoBloom-B.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/mpware/Documents/NOTEBOOKS/CDC/./code/src/cdc/features/extractor.py:40: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pretrained = torch.load(modelpath, map_location=torch.device('cpu'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing model, dinov2_vitb14/DinoBloom-B.pth device: cuda\n",
      "Freezing full model\n",
      "Unfreezing -5 blocks, last 5 over 12\n",
      "Override prepare stage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/mpware/Documents/NOTEBOOKS/CDC/./code/src/cdc/script/inferv1.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_dump = torch.load(best_weights, map_location='cpu')\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble of 1 model(s), tta=[[<function hflip at 0x7fae90bcb400>]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce77d00f87854854a0230c4c256ec815",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: ./models/TRUSTII-RGB/foundation_dinov2_vitb14_DinoBloom-B.pth_224_None_v1.4.0.6-pl-crop-m16/stage1/seed42/fold1/best_epoch=30-val_f1=0.9170.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/mpware/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading (dinov2_vitb14) weights: DinoBloom-B.pth\n",
      "Freezing model, dinov2_vitb14/DinoBloom-B.pth device: cuda\n",
      "Freezing full model\n",
      "Unfreezing -5 blocks, last 5 over 12\n",
      "Override prepare stage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble of 1 model(s), tta=[[<function hflip at 0x7fae90bcb400>]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c26fb2cb9b948c0808bef843965ab5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: ./models/TRUSTII-RGB/foundation_dinov2_vitb14_DinoBloom-B.pth_224_None_v1.4.0.6-pl-crop-m16/stage1/seed42/fold2/best_epoch=29-val_f1=0.9129.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/mpware/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading (dinov2_vitb14) weights: DinoBloom-B.pth\n",
      "Freezing model, dinov2_vitb14/DinoBloom-B.pth device: cuda\n",
      "Freezing full model\n",
      "Unfreezing -5 blocks, last 5 over 12\n",
      "Override prepare stage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble of 1 model(s), tta=[[<function hflip at 0x7fae90bcb400>]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25db35b52f4842328717754cc7966baf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: ./models/TRUSTII-RGB/foundation_dinov2_vitb14_DinoBloom-B.pth_224_None_v1.4.0.6-pl-crop-m16/stage1/seed42/fold3/best_epoch=28-val_f1=0.9095.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/mpware/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading (dinov2_vitb14) weights: DinoBloom-B.pth\n",
      "Freezing model, dinov2_vitb14/DinoBloom-B.pth device: cuda\n",
      "Freezing full model\n",
      "Unfreezing -5 blocks, last 5 over 12\n",
      "Override prepare stage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble of 1 model(s), tta=[[<function hflip at 0x7fae90bcb400>]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a2ba140a01e493094822cbe97678650",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 432.63022470474243\n",
      "Executing multi-classes model: {'cnn_and_transformers-bb-multiclass': {'root_dir': './models/TRUSTII-RGB/timm_nextvit_large.bd_ssld_6m_in1k_384_384_None_v1.4.0-pl-crop-m16/stage2/seed42'}} (22689, 14) TTA: [[<function hflip at 0x7fae90bcb400>]]\n",
      "Loading: ./models/TRUSTII-RGB/timm_nextvit_large.bd_ssld_6m_in1k_384_384_None_v1.4.0-pl-crop-m16/stage2/seed42/fold0/best_epoch=23-val_f1=0.9154.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/mpware/Documents/NOTEBOOKS/CDC/./code/src/cdc/script/inferv1.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_dump = torch.load(best_weights, map_location='cpu')\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble of 1 model(s), tta=[[<function hflip at 0x7fae90bcb400>]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "647b263de3c04895b1e6fe5d12884ed7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: ./models/TRUSTII-RGB/timm_nextvit_large.bd_ssld_6m_in1k_384_384_None_v1.4.0-pl-crop-m16/stage2/seed42/fold1/best_epoch=22-val_f1=0.9217.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble of 1 model(s), tta=[[<function hflip at 0x7fae90bcb400>]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1285b6d14464ade8a26d35e1218dffb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: ./models/TRUSTII-RGB/timm_nextvit_large.bd_ssld_6m_in1k_384_384_None_v1.4.0-pl-crop-m16/stage2/seed42/fold2/best_epoch=21-val_f1=0.9153.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble of 1 model(s), tta=[[<function hflip at 0x7fae90bcb400>]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "216e6514120b429d956e4843bae138a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: ./models/TRUSTII-RGB/timm_nextvit_large.bd_ssld_6m_in1k_384_384_None_v1.4.0-pl-crop-m16/stage2/seed42/fold3/best_epoch=23-val_f1=0.9138.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble of 1 model(s), tta=[[<function hflip at 0x7fae90bcb400>]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba948371b8ef4276b5641ee849334032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 1347.8589000701904\n",
      "Executing multi-classes model: {'cnn_and_transformers-bb-multiclass': {'root_dir': './models/TRUSTII-RGB/timm_tiny_vit_21m_512.dist_in22k_ft_in1k_512_None_v1.5.0-pl-crop-m16/stage1/seed42'}} (22689, 14) TTA: None\n",
      "Loading: ./models/TRUSTII-RGB/timm_tiny_vit_21m_512.dist_in22k_ft_in1k_512_None_v1.5.0-pl-crop-m16/stage1/seed42/fold0/best_epoch=35-val_f1=0.9192.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/mpware/Documents/NOTEBOOKS/CDC/./code/src/cdc/script/inferv1.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_dump = torch.load(best_weights, map_location='cpu')\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6a9beb5d6a644fcbbd678d8ced11632",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: ./models/TRUSTII-RGB/timm_tiny_vit_21m_512.dist_in22k_ft_in1k_512_None_v1.5.0-pl-crop-m16/stage1/seed42/fold1/best_epoch=31-val_f1=0.9235.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2588f79f4c1d4550a340e3bcec802fd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: ./models/TRUSTII-RGB/timm_tiny_vit_21m_512.dist_in22k_ft_in1k_512_None_v1.5.0-pl-crop-m16/stage1/seed42/fold2/best_epoch=32-val_f1=0.9174.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4083f3129b124b79a7cb7b961042d69d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: ./models/TRUSTII-RGB/timm_tiny_vit_21m_512.dist_in22k_ft_in1k_512_None_v1.5.0-pl-crop-m16/stage1/seed42/fold3/best_epoch=34-val_f1=0.9127.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0edc571a388411fa2cce48a34a2238a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 633.6524655818939\n",
      "Executing multi-labels model: {'cnn_and_transformers-multilabel': {'root_dir': './models/TRUSTII-RGB/timm_tf_efficientnetv2_m.in21k_512_None_v1.3.0-pl/stage3/seed42'}} (22689, 3) TTA: [[<function hflip at 0x7fae90bcb400>]]\n",
      "Loading: ./models/TRUSTII-RGB/timm_tf_efficientnetv2_m.in21k_512_None_v1.3.0-pl/stage3/seed42/fold0/best_epoch=22-val_f1=0.9300.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/mpware/Documents/NOTEBOOKS/CDC/./code/src/cdc/script/inferv1.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_dump = torch.load(best_weights, map_location='cpu')\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble of 1 model(s), tta=[[<function hflip at 0x7fae90bcb400>]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3413c59564a5439bbd7bf2521fbf8106",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: ./models/TRUSTII-RGB/timm_tf_efficientnetv2_m.in21k_512_None_v1.3.0-pl/stage3/seed42/fold1/best_epoch=21-val_f1=0.9343.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble of 1 model(s), tta=[[<function hflip at 0x7fae90bcb400>]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa02edac12bf44d7bb65fabdf5441fb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: ./models/TRUSTII-RGB/timm_tf_efficientnetv2_m.in21k_512_None_v1.3.0-pl/stage3/seed42/fold2/best_epoch=22-val_f1=0.9302.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble of 1 model(s), tta=[[<function hflip at 0x7fae90bcb400>]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30ab109ff1ca4e8aba6c008c7dfee259",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: ./models/TRUSTII-RGB/timm_tf_efficientnetv2_m.in21k_512_None_v1.3.0-pl/stage3/seed42/fold3/best_epoch=23-val_f1=0.9289.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble of 1 model(s), tta=[[<function hflip at 0x7fae90bcb400>]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c78f7a2b7add42ec9760ebd9b3c63c89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 1158.1916420459747\n",
      "CPU times: user 1h 34min 56s, sys: 10min 49s, total: 1h 45min 45s\n",
      "Wall time: 1h 43min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for name, info in models_dict.items():\n",
    "    root_dirs = [c for c in info.keys() if c. startswith('root_dir')]    \n",
    "    for root_dir_name in root_dirs:\n",
    "        root_dir = info.get(root_dir_name)        \n",
    "        if isinstance(root_dir, list):\n",
    "            for root_dir_, tta in root_dir:\n",
    "                models_pl = {name: {\"root_dir\": root_dir_}}\n",
    "                if '-bb-multiclass' in name:\n",
    "                    test_pd = pd.read_parquet(CROP_FILE_TEST_CLEANED).reset_index(drop=True) # .head(512)\n",
    "                    print(\"Executing multi-classes model:\", models_pl, test_pd.shape, \"TTA:\", tta)\n",
    "                    final_test_pd = infer_model(models_pl, test_pd, tta=tta, images_home=CROP_HOME_TEST)                \n",
    "                    final_test_pd.to_parquet(os.path.join(root_dir_, \"test_predictions_%s.parquet\"%INFERENCE_NAME))\n",
    "                    # Debug\n",
    "                    logits_col = [c for c in final_test_pd.columns if \"logits_\" in c]\n",
    "                    if len(logits_col) > 23:        \n",
    "                        final_test_pd[\"preds\"] = final_test_pd[logits_col[0:23]].values.argmax(axis=1).astype(np.int32)                    \n",
    "                    submission_csv_pd = final_test_pd[[\"trustii_id\", \"NAME\", \"pred_x1\", \"pred_y1\", \"pred_x2\", \"pred_y2\", \"preds\"]].copy().rename(columns={'pred_x1':'x1', 'pred_y1':'y1', 'pred_x2':'x2', 'pred_y2':'y2', 'preds':'class'})\n",
    "                    submission_csv_pd[\"x1\"] = submission_csv_pd[\"x1\"].astype(np.int32)\n",
    "                    submission_csv_pd[\"y1\"] = submission_csv_pd[\"y1\"].astype(np.int32)\n",
    "                    submission_csv_pd[\"x2\"] = submission_csv_pd[\"x2\"].astype(np.int32)\n",
    "                    submission_csv_pd[\"y2\"] = submission_csv_pd[\"y2\"].astype(np.int32)\n",
    "                    submission_csv_pd[\"class\"] = submission_csv_pd[\"class\"].astype(np.int32)\n",
    "                    submission_csv_pd[\"class\"] = submission_csv_pd[\"class\"].map(class_mapping)\n",
    "                    submission_csv_pd = pd.merge(pd.read_csv(TEST_FILE), submission_csv_pd, on=[\"trustii_id\", \"NAME\"], how=\"left\")\n",
    "                    submission_csv_pd.to_csv(os.path.join(root_dir_, \"submission_%s.csv\"%INFERENCE_NAME), index=False)\n",
    "                elif '-multilabel' in name:\n",
    "                    test_pd = pd.read_csv(TEST_FILE) # .head(512)\n",
    "                    test_pd[\"filename\"] = test_pd[\"NAME\"]           \n",
    "                    print(\"Executing multi-labels model:\", models_pl, test_pd.shape, \"TTA:\", tta)\n",
    "                    final_test_pd = infer_model(models_pl, test_pd, tta=tta, images_home=TEST_HOME)\n",
    "                    final_test_pd.to_parquet(os.path.join(root_dir_, \"test_predictions_%s.parquet\"%INFERENCE_NAME))                     \n",
    "        else:\n",
    "            raise(Exception(\"List expected:%s\"%root_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3b9a0f-987c-416f-ae40-09a2b1307c0a",
   "metadata": {},
   "source": [
    "### Ensemble models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ebb01ae-7619-460a-9e38-d2bdd7ac1dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert logits to probabilities and average them\n",
    "def ensemble_multiclass_multilabels(multiclass_single_wbc_pd, multilabels_pd, weights=[0.5, 0.5]):\n",
    "    # Move from logits to probabilities (softmax because of multiclasses) - Need to be followed by argmax\n",
    "    logits_col = [c for c in multiclass_single_wbc_pd.columns if \"logits_\" in c][0:23]\n",
    "    sprobs_col = [c.replace(\"logits_\", \"sprobs_\") for c in multiclass_single_wbc_pd.columns if \"logits_\" in c][0:23]\n",
    "    multiclass_single_wbc_pd[sprobs_col] = torch.softmax(torch.from_numpy(multiclass_single_wbc_pd[logits_col].values), dim=1).numpy()\n",
    "    sprobs_pd = multiclass_single_wbc_pd[[\"NAME\"] + sprobs_col]\n",
    "    # display(sprobs_pd)\n",
    "\n",
    "    # Move from logits to probabilities (sigmoid because of multilabels) - Need to be followed by threshold\n",
    "    logits_col = [c for c in multilabels_pd.columns if \"logits_\" in c][0:23]\n",
    "    mprobs_col = [c.replace(\"logits_\", \"mprobs_\") for c in multilabels_pd.columns if \"logits_\" in c][0:23]\n",
    "    multilabels_pd[mprobs_col] = torch.sigmoid(torch.from_numpy(multilabels_pd[logits_col].values)).numpy()\n",
    "    mprobs_pd = multilabels_pd[[\"NAME\"] + mprobs_col]\n",
    "    # display(mprobs_pd)\n",
    "\n",
    "    # Merge both on NAME and ensemble with average\n",
    "    probs_pd = pd.merge(sprobs_pd, mprobs_pd, on=\"NAME\", how=\"inner\")\n",
    "    # print(probs_pd.shape)\n",
    "    probs_pd[sprobs_col].shape, probs_pd[mprobs_col].shape\n",
    "    ensemble_probs = [probs_pd[sprobs_col].values, probs_pd[mprobs_col].values]\n",
    "    # ensemble_probs = [probs_pd[mprobs_col].values]\n",
    "    print(probs_pd[sprobs_col].values.shape, probs_pd[mprobs_col].values.shape)\n",
    "    # print(\"Ensemble of:\", np.stack(ensemble_probs).shape, weights)\n",
    "    # ensemble_probs = np.nanmean(np.stack(ensemble_probs), axis=0)\n",
    "    ensemble_probs = np.average(np.stack(ensemble_probs), axis=0, weights=weights)\n",
    "    \n",
    "    # print(ensemble_probs.shape)\n",
    "    probs_col = [c.replace(\"sprobs_\", \"eprobs_\") for c in probs_pd.columns if \"sprobs_\" in c]\n",
    "    probs_pd[probs_col] = ensemble_probs\n",
    "    # display(probs_pd[[\"NAME\"] + probs_col])\n",
    "\n",
    "    return probs_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dca8074b-0869-40ae-9db1-9b9a48f517a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: root_dir_mc_512/0.25 [('./models/TRUSTII-RGB/timm_tf_efficientnetv2_m.in21k_512_None_v1.4.0-pl-crop-m16/stage2/seed42', None)] (22689, 38)\n",
      "Loading: root_dir_mc_224/0.25 [('./models/TRUSTII-RGB/timm_vit_large_patch16_224.augreg_in21k_ft_in1k_224_None_v1.2.5-pl-crop-m16/stage1/seed42', [[<function hflip at 0x7fae90bcb400>]]), ('./models/TRUSTII-RGB/timm_vit_large_patch16_224.augreg_in21k_ft_in1k_224_None_v1.2.4-pl-crop-m16/stage1/seed42', [[<function hflip at 0x7fae90bcb400>]]), ('./models/TRUSTII-RGB/foundation_dinov2_vitb14_DinoBloom-B.pth_224_None_v1.4.0.6-pl-crop-m16/stage1/seed42', [[<function hflip at 0x7fae90bcb400>]])] (22689, 38)\n",
      "Loading: root_dir_mc_384/0.25 [('./models/TRUSTII-RGB/timm_nextvit_large.bd_ssld_6m_in1k_384_384_None_v1.4.0-pl-crop-m16/stage2/seed42', [[<function hflip at 0x7fae90bcb400>]])] (22689, 38)\n",
      "Loading: root_dir_mc_tr_512/0.25 [('./models/TRUSTII-RGB/timm_tiny_vit_21m_512.dist_in22k_ft_in1k_512_None_v1.5.0-pl-crop-m16/stage1/seed42', None)] (22689, 38)\n",
      "Ensemble(x4): cnn_and_transformers-bb-multiclass (22689, 38) Weights: [0.25, 0.25, 0.25, 0.25]\n",
      "Loading: root_dir_ml_512/1.0 [('./models/TRUSTII-RGB/timm_tf_efficientnetv2_m.in21k_512_None_v1.3.0-pl/stage3/seed42', [[<function hflip at 0x7fae90bcb400>]])] (22689, 51)\n",
      "Ensemble(x1): cnn_and_transformers-multilabel (20751, 50) Weights: [1.0]\n",
      "CPU times: user 332 ms, sys: 32.1 ms, total: 364 ms\n",
      "Wall time: 409 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "thr = 0.5\n",
    "best_thrs = None\n",
    "for name, info in models_dict.items():\n",
    "    root_dirs = [c for c in info.keys() if c. startswith('root_dir')]\n",
    "    root_weights = [float(x.split(\"/\")[-1]) for x in root_dirs]\n",
    "    root_dirs_ensemble_logits = []\n",
    "    for root_dir_name in root_dirs:\n",
    "        root_dir = info.get(root_dir_name)\n",
    "        if isinstance(root_dir, list):\n",
    "            ensemble_logits = []\n",
    "            for root_dir_, _ in root_dir:\n",
    "                test_pd_ = pd.read_parquet(os.path.join(root_dir_, \"test_predictions_%s.parquet\"%INFERENCE_NAME))\n",
    "                logits_col = [c for c in test_pd_.columns if \"logits_\" in c][0:23]\n",
    "                ensemble_logits.append(test_pd_[logits_col].values)\n",
    "            ensemble_logits = np.nanmean(np.stack(ensemble_logits), axis=0)\n",
    "            # Ensemble OOF with new logits and predictions\n",
    "            test_pd = test_pd_.copy()\n",
    "            test_pd[logits_col] = ensemble_logits\n",
    "            if LABEL in name:\n",
    "                test_pd[[c for c in test_pd.columns if \"preds_\" in c][0:23]] = (torch.sigmoid(torch.from_numpy(ensemble_logits)).numpy() > thr).astype(int)\n",
    "            else:\n",
    "                test_pd[\"preds\"] = ensemble_logits.argmax(1)\n",
    "        else:\n",
    "            raise(Exception(\"List expected:%s\"%root_dir))\n",
    "        root_dirs_ensemble_logits.append(test_pd[logits_col].values)\n",
    "        print(\"Loading:\", root_dir_name, root_dir, test_pd.shape)\n",
    "    \n",
    "    # root_dirs_ensemble_logits = np.nanmean(np.stack(root_dirs_ensemble_logits), axis=0)\n",
    "    root_dirs_ensemble_logits = np.average(np.stack(root_dirs_ensemble_logits), axis=0, weights=root_weights)\n",
    "    test_pd[logits_col] = root_dirs_ensemble_logits \n",
    "    if LABEL in name:\n",
    "        test_pd[[c for c in test_pd.columns if \"preds_\" in c][0:23]] = (torch.sigmoid(torch.from_numpy(root_dirs_ensemble_logits)).numpy()> thr).astype(int)\n",
    "    else:\n",
    "        test_pd[\"preds\"] = test_pd[logits_col].values.argmax(1)\n",
    "            \n",
    "    if LABEL in name:\n",
    "        test_pd = test_pd.drop(columns=[\"trustii_id\"]).groupby([\"NAME\",\"filename\"]).first().reset_index()\n",
    "    \n",
    "    print(\"Ensemble(x%d):\"%len(root_dirs), name, test_pd.shape, \"Weights:\", root_weights)\n",
    "    # display(test_pd.head())\n",
    "    info[\"test_pd\"] = test_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07455bc2-10bf-41cd-87d9-4ecd0bf1423a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images with single WBC (19373, 39) 85.38%\n",
      "(19373, 23) (19373, 23)\n",
      "Threshold 0.50 predictions has 4 WBC with multilabels\n",
      "Changes argmax (251): 1.30%\n",
      "CPU times: user 89.4 ms, sys: 2.7 ms, total: 92.1 ms\n",
      "Wall time: 66.7 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NAME</th>\n",
       "      <th>preds</th>\n",
       "      <th>preds_ensemble_argmax</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000455d4-8.jpg</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0007ccec-2.jpg</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00080027-c.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00084489-e.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000cfe84-e.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19368</th>\n",
       "      <td>ffef4aae-c.jpg</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19369</th>\n",
       "      <td>fff4d913-4.jpg</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19370</th>\n",
       "      <td>fff50857-a.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19371</th>\n",
       "      <td>fff97dfe-0.jpg</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19372</th>\n",
       "      <td>fff98fc2-4.jpg</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19373 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 NAME  preds  preds_ensemble_argmax\n",
       "0      000455d4-8.jpg     19                     19\n",
       "1      0007ccec-2.jpg     18                     18\n",
       "2      00080027-c.jpg      0                      0\n",
       "3      00084489-e.jpg      1                      1\n",
       "4      000cfe84-e.jpg      3                      3\n",
       "...               ...    ...                    ...\n",
       "19368  ffef4aae-c.jpg     12                     12\n",
       "19369  fff4d913-4.jpg      8                      8\n",
       "19370  fff50857-a.jpg      0                      0\n",
       "19371  fff97dfe-0.jpg     15                     15\n",
       "19372  fff98fc2-4.jpg     13                     13\n",
       "\n",
       "[19373 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "multiclasses_key = \"cnn_and_transformers-bb-multiclass\"\n",
    "multilabels_key = \"cnn_and_transformers-multilabel\"\n",
    "multilabels_pd_ = models_dict[multilabels_key][\"test_pd\"] # Image level\n",
    "multiclass_pd_ = models_dict[multiclasses_key][\"test_pd\"] # Box level\n",
    "multiclass_pd_[\"wbc\"] = multiclass_pd_.groupby([\"NAME\"])[\"trustii_id\"].transform('count')\n",
    "multiclass_single_wbc_pd_ = multiclass_pd_[multiclass_pd_[\"wbc\"] == 1].reset_index(drop=True)\n",
    "print(\"Images with single WBC\", multiclass_single_wbc_pd_.shape, \"%.2f%%\"%(multiclass_single_wbc_pd_.shape[0]*100/multiclass_pd_.shape[0]))\n",
    "# Run ensemble\n",
    "ensemble_probs_pd = ensemble_multiclass_multilabels(multiclass_single_wbc_pd_, multilabels_pd_, weights=[ALPHA, 1-ALPHA])\n",
    "eprobs_col = [c for c in ensemble_probs_pd.columns if \"eprobs_\" in c]\n",
    "multiclass_single_wbc_ensemble_pd = pd.merge(multiclass_single_wbc_pd_, ensemble_probs_pd[[\"NAME\"] + eprobs_col], on=\"NAME\", how=\"inner\")\n",
    "mpreds = (multiclass_single_wbc_ensemble_pd[eprobs_col].values > thr).astype(int)\n",
    "print(\"Threshold %.2f predictions has %d WBC with multilabels\" % (thr, np.sum(np.sum(mpreds, axis=1) > 1)))\n",
    "multiclass_single_wbc_ensemble_pd[\"preds_ensemble_argmax\"] = multiclass_single_wbc_ensemble_pd[eprobs_col].values.argmax(axis=1)\n",
    "multiclass_single_wbc_ensemble_pd[\"preds\"] = multiclass_single_wbc_ensemble_pd[\"preds\"].astype(np.int32)\n",
    "changes_argmax_pd = multiclass_single_wbc_ensemble_pd[multiclass_single_wbc_ensemble_pd[\"preds\"] != multiclass_single_wbc_ensemble_pd[\"preds_ensemble_argmax\"]][[\"NAME\", \"preds\", \"preds_ensemble_argmax\"]]\n",
    "print(\"Changes argmax (%d): %.2f%%\" % (changes_argmax_pd.shape[0], changes_argmax_pd.shape[0]*100/multiclass_single_wbc_ensemble_pd.shape[0]))\n",
    "\n",
    "if best_thrs is not None:\n",
    "    # Apply best threshold per label\n",
    "    for binary_class in all_classes:\n",
    "        thr_ = best_thrs[binary_class]\n",
    "        mpreds = (multiclass_single_wbc_ensemble_pd[\"eprobs_%d\"%binary_class].values > thr_).astype(int)\n",
    "        multiclass_single_wbc_ensemble_pd[\"epreds_%d\"%binary_class] = mpreds\n",
    "    multiclass_single_wbc_ensemble_pd[\"preds_ensemble_thr\"] = multiclass_single_wbc_ensemble_pd[[\"epreds_%d\"%c for c in all_classes]].values.argmax(axis=1)\n",
    "    for binary_class in all_classes:\n",
    "        del multiclass_single_wbc_ensemble_pd[\"epreds_%d\"%binary_class]    \n",
    "    changes_thr_pd = multiclass_single_wbc_ensemble_pd[multiclass_single_wbc_ensemble_pd[\"preds\"] != multiclass_single_wbc_ensemble_pd[\"preds_ensemble_thr\"]][[\"NAME\", \"preds\", \"preds_ensemble_thr\"]]\n",
    "    print(\"Changes thr (%d): %.2f%%\" % (changes_thr_pd.shape[0], changes_thr_pd.shape[0]*100/multiclass_single_wbc_ensemble_pd.shape[0]))\n",
    "\n",
    "preds_ensembles_cols =  [c for c in multiclass_single_wbc_ensemble_pd.columns if \"preds_ensemble_\" in c]\n",
    "multiclass_single_wbc_ensemble_pd[[\"NAME\", \"preds\"] + preds_ensembles_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afc6fd09-877f-4c1f-b2f8-90d307a65818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.8 ms, sys: 644 µs, total: 16.5 ms\n",
      "Wall time: 16.6 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trustii_id</th>\n",
       "      <th>NAME</th>\n",
       "      <th>pred_x1</th>\n",
       "      <th>pred_y1</th>\n",
       "      <th>pred_x2</th>\n",
       "      <th>pred_y2</th>\n",
       "      <th>preds_ensemble_argmax</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23798</td>\n",
       "      <td>000455d4-8.jpg</td>\n",
       "      <td>93</td>\n",
       "      <td>98</td>\n",
       "      <td>266</td>\n",
       "      <td>265</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22386</td>\n",
       "      <td>0007ccec-2.jpg</td>\n",
       "      <td>91</td>\n",
       "      <td>82</td>\n",
       "      <td>272</td>\n",
       "      <td>283</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59769</td>\n",
       "      <td>00080027-c.jpg</td>\n",
       "      <td>111</td>\n",
       "      <td>107</td>\n",
       "      <td>257</td>\n",
       "      <td>261</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>61484</td>\n",
       "      <td>00084489-e.jpg</td>\n",
       "      <td>118</td>\n",
       "      <td>115</td>\n",
       "      <td>241</td>\n",
       "      <td>269</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36896</td>\n",
       "      <td>000cfe84-e.jpg</td>\n",
       "      <td>100</td>\n",
       "      <td>91</td>\n",
       "      <td>251</td>\n",
       "      <td>264</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22684</th>\n",
       "      <td>45140</td>\n",
       "      <td>fffff491-3.jpg</td>\n",
       "      <td>132</td>\n",
       "      <td>126</td>\n",
       "      <td>242</td>\n",
       "      <td>244</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22685</th>\n",
       "      <td>16280</td>\n",
       "      <td>fffff491-3.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>42</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22686</th>\n",
       "      <td>44984</td>\n",
       "      <td>fffff491-3.jpg</td>\n",
       "      <td>334</td>\n",
       "      <td>3</td>\n",
       "      <td>359</td>\n",
       "      <td>94</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22687</th>\n",
       "      <td>41305</td>\n",
       "      <td>fffff491-3.jpg</td>\n",
       "      <td>277</td>\n",
       "      <td>63</td>\n",
       "      <td>356</td>\n",
       "      <td>148</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22688</th>\n",
       "      <td>42293</td>\n",
       "      <td>fffff491-3.jpg</td>\n",
       "      <td>66</td>\n",
       "      <td>135</td>\n",
       "      <td>129</td>\n",
       "      <td>244</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22689 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       trustii_id            NAME  pred_x1  pred_y1  pred_x2  pred_y2  \\\n",
       "0           23798  000455d4-8.jpg       93       98      266      265   \n",
       "1           22386  0007ccec-2.jpg       91       82      272      283   \n",
       "2           59769  00080027-c.jpg      111      107      257      261   \n",
       "3           61484  00084489-e.jpg      118      115      241      269   \n",
       "4           36896  000cfe84-e.jpg      100       91      251      264   \n",
       "...           ...             ...      ...      ...      ...      ...   \n",
       "22684       45140  fffff491-3.jpg      132      126      242      244   \n",
       "22685       16280  fffff491-3.jpg        0        0       46       42   \n",
       "22686       44984  fffff491-3.jpg      334        3      359       94   \n",
       "22687       41305  fffff491-3.jpg      277       63      356      148   \n",
       "22688       42293  fffff491-3.jpg       66      135      129      244   \n",
       "\n",
       "       preds_ensemble_argmax  \n",
       "0                         19  \n",
       "1                         18  \n",
       "2                          0  \n",
       "3                          1  \n",
       "4                          3  \n",
       "...                      ...  \n",
       "22684                     11  \n",
       "22685                     11  \n",
       "22686                     11  \n",
       "22687                      7  \n",
       "22688                      7  \n",
       "\n",
       "[22689 rows x 7 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Merge back to get final predictions\n",
    "ensemble_multiclass_pd = pd.merge(multiclass_pd_, multiclass_single_wbc_ensemble_pd[[\"NAME\"] + preds_ensembles_cols], on='NAME', how='left')\n",
    "ensemble_multiclass_pd.loc[ensemble_multiclass_pd[\"wbc\"] > 1, \"preds_ensemble_argmax\"] = ensemble_multiclass_pd[\"preds\"]\n",
    "ensemble_multiclass_pd[\"preds_ensemble_argmax\"] = ensemble_multiclass_pd[\"preds_ensemble_argmax\"].astype('Int32')\n",
    "if 'preds_ensemble_thr' in ensemble_multiclass_pd.columns:\n",
    "    ensemble_multiclass_pd.loc[ensemble_multiclass_pd[\"wbc\"] > 1, \"preds_ensemble_thr\"] = ensemble_multiclass_pd[\"preds\"]\n",
    "    ensemble_multiclass_pd[\"preds_ensemble_thr\"] = ensemble_multiclass_pd[\"preds_ensemble_thr\"].astype('Int32')\n",
    "ensemble_multiclass_pd[[\"trustii_id\", \"NAME\", \"pred_x1\", \"pred_y1\", \"pred_x2\", \"pred_y2\"] + preds_ensembles_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f77a588e-3d72-48b5-8afd-8b376a36dd51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trustii_id</th>\n",
       "      <th>NAME</th>\n",
       "      <th>x1</th>\n",
       "      <th>y1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y2</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>43232</td>\n",
       "      <td>681daf42-3.jpg</td>\n",
       "      <td>116</td>\n",
       "      <td>116</td>\n",
       "      <td>248</td>\n",
       "      <td>242</td>\n",
       "      <td>LLC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>65979</td>\n",
       "      <td>172bf8a5-e.jpg</td>\n",
       "      <td>100</td>\n",
       "      <td>96</td>\n",
       "      <td>229</td>\n",
       "      <td>269</td>\n",
       "      <td>Lysee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60083</td>\n",
       "      <td>179a21ee-4.jpg</td>\n",
       "      <td>102</td>\n",
       "      <td>94</td>\n",
       "      <td>273</td>\n",
       "      <td>278</td>\n",
       "      <td>PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7302</td>\n",
       "      <td>f15e265d-6.jpg</td>\n",
       "      <td>101</td>\n",
       "      <td>99</td>\n",
       "      <td>261</td>\n",
       "      <td>267</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31846</td>\n",
       "      <td>94cbe9cc-3.jpg</td>\n",
       "      <td>118</td>\n",
       "      <td>112</td>\n",
       "      <td>244</td>\n",
       "      <td>254</td>\n",
       "      <td>PNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22684</th>\n",
       "      <td>13601</td>\n",
       "      <td>7d1b6d4a-3.jpg</td>\n",
       "      <td>10</td>\n",
       "      <td>156</td>\n",
       "      <td>88</td>\n",
       "      <td>233</td>\n",
       "      <td>LF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22685</th>\n",
       "      <td>62911</td>\n",
       "      <td>33b1749a-b.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>34</td>\n",
       "      <td>LyB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22686</th>\n",
       "      <td>16139</td>\n",
       "      <td>d11c35af-a.jpg</td>\n",
       "      <td>41</td>\n",
       "      <td>145</td>\n",
       "      <td>171</td>\n",
       "      <td>284</td>\n",
       "      <td>MoB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22687</th>\n",
       "      <td>47444</td>\n",
       "      <td>d0fb06c0-3.jpg</td>\n",
       "      <td>102</td>\n",
       "      <td>112</td>\n",
       "      <td>262</td>\n",
       "      <td>249</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22688</th>\n",
       "      <td>55334</td>\n",
       "      <td>9573b540-c.jpg</td>\n",
       "      <td>109</td>\n",
       "      <td>126</td>\n",
       "      <td>259</td>\n",
       "      <td>278</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22689 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       trustii_id            NAME   x1   y1   x2   y2  class\n",
       "0           43232  681daf42-3.jpg  116  116  248  242    LLC\n",
       "1           65979  172bf8a5-e.jpg  100   96  229  269  Lysee\n",
       "2           60083  179a21ee-4.jpg  102   94  273  278     PM\n",
       "3            7302  f15e265d-6.jpg  101   99  261  267      M\n",
       "4           31846  94cbe9cc-3.jpg  118  112  244  254    PNN\n",
       "...           ...             ...  ...  ...  ...  ...    ...\n",
       "22684       13601  7d1b6d4a-3.jpg   10  156   88  233     LF\n",
       "22685       62911  33b1749a-b.jpg    2    1   89   34    LyB\n",
       "22686       16139  d11c35af-a.jpg   41  145  171  284    MoB\n",
       "22687       47444  d0fb06c0-3.jpg  102  112  262  249      B\n",
       "22688       55334  9573b540-c.jpg  109  126  259  278      B\n",
       "\n",
       "[22689 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 47.5 ms, sys: 2.38 ms, total: 49.9 ms\n",
      "Wall time: 75.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Submission file (argmax)\n",
    "submission_csv_pd = ensemble_multiclass_pd[[\"trustii_id\", \"NAME\", \"pred_x1\", \"pred_y1\", \"pred_x2\", \"pred_y2\", \"preds_ensemble_argmax\"]].copy().rename(columns={'pred_x1':'x1', 'pred_y1':'y1', 'pred_x2':'x2', 'pred_y2':'y2', 'preds_ensemble_argmax':'class'})\n",
    "submission_csv_pd[\"class\"] = submission_csv_pd[\"class\"].astype('Int32')\n",
    "submission_csv_pd[\"class\"] = submission_csv_pd[\"class\"].map(class_mapping)\n",
    "# Keep same order\n",
    "submission_csv_pd = pd.merge(pd.read_csv(TEST_FILE), submission_csv_pd, on=[\"trustii_id\", \"NAME\"], how=\"left\")\n",
    "folder = os.path.join(\"submissions\", INFERENCE_NAME, \"_\".join(models_dict.keys()))\n",
    "os.makedirs(folder, exist_ok=True)\n",
    "submission_csv_pd.to_csv(os.path.join(folder, \"submission_argmax.csv\"), index=False)\n",
    "display(submission_csv_pd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
